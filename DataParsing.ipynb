{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"exit()\n!pip3 install bs4\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\ncount = 100\n#Webpage variable, changing for every 100 entries.\noffset = [0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400]\n#Initialized empty arrays to store data\nlist_rows = []\nlist_heads = []\n\n#Loop to parse multiple webpage, for data\nfor offset_url in offset:\n\tr = requests.get('https://ca.finance.yahoo.com/mutualfunds?offset=' + str(offset_url) + '&count=' + str(count))\n\tsoup = BeautifulSoup(r.text, 'html.parser')\n\n\t#Loop to select tables on each webpage\t\n\tfor row in soup.select('table tbody'):\n\t\trows = row.find_all('tr')\t#Find all tr tags\n\t\t#Loop to find data of the rows\n\t\tfor row in rows:\n\t\t\trow_td = row.find_all('td')\t#Find all td tags\n\t\t\tstr_row = str(row_td)\t\t#Convert the raw data in string datatype\n\t\t\tclean = re.compile('<.*?>')\t#Find and match all tags (content between < and >)\n\t\t\tcleanrow = re.sub(clean, '',str_row)\t#Delete all the useless data between the tags and store only the valuable data\n\t\t\tlist_rows.append(cleanrow)\t#Append the data\n\t\tdf_row = pd.DataFrame(list_rows)\t#Convert the data into pandas dataframe to process it further\n        \n        \n#Request the URL again to get the headers\nr = requests.get('https://ca.finance.yahoo.com/mutualfunds')\nsoup = BeautifulSoup(r.text, 'html.parser')\n#Loopfor all the headers\nfor head in soup.select('table thead'):\n\theads = head.find_all('tr')\t#Find all tr tags\n\tfor head in heads:\n\t\thead_th = head.find_all('th')\t#Find all th tags to get the headers\n\t\tstr_head = str(head_th)\t\t#Convert the raw data to string datatype\n\t\tclean = re.compile('<.*?>')\t#Find and match all tags (content between < and >)\n\t\tcleanhead = (re.sub(clean, '',str_head))\t#Delete all the useless data between the tags and store only the valuable data\n\t\tlist_heads.append(cleanhead)\t#Append the headers\n\tdf_head = pd.DataFrame(list_heads)\t#Convert the data to pandas dataframe to process it further\n    \ndf1 = df_row[0].str.split(',', expand=True)\t#Split the rows and make columns at each ','\ndf2 = df_head[0].str.split(',', expand=True)\nframes = [df2,df1]\t\t#Merged the two lists, df1 and df2\ndf_merged = pd.concat(frames)\t#Converted the list in dataframe\ndf_merged[0] = df_merged[0].str.strip('[')\t#Removed useless character '[' from the first column\ndf_merged = df_merged.rename(columns=df_merged.iloc[0])\t#Replaced the column indices with the column names\ndf_merged = df_merged.drop(df_merged.index[0])\t\t#Dropped the indices column for rows\ndf_merged.to_csv('Data.csv', index=False)\t\t#Converted the dataframe to .csv format and made a file\n\nprint ('Done')\t\t#Printed a message to show that everything was done successfully without error","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: bs4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (0.0.1)\nRequirement already satisfied: beautifulsoup4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from bs4) (4.9.3)\nRequirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0.1)\nDone\n"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}